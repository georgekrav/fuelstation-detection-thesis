# -*- coding: utf-8 -*-
"""
Additional Geocoding Experiments - ÎšÎ±Î¸Î·Î³Î·Ï„Î®Ï‚ Requests
Î ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹ Î½Î­ÎµÏ‚ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï…Ï‚ ÏƒÏ„Î¿ existing geocoding_19methods_full.xlsx
"""

import os
import pandas as pd
import numpy as np
import re
from tqdm import tqdm
import time
import googlemaps
from math import radians, cos, sin, asin, sqrt
from dotenv import load_dotenv

# ============= CONFIGURATION =============
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_MAPS_API_KEY')
gmaps = googlemaps.Client(key=GOOGLE_API_KEY)

BASE_DIR = "/Users/geo/Desktop/fuelstation-detection-thesis/data/geocoding_pattern_analysis_ml"
EXISTING_RESULTS = os.path.join(BASE_DIR, "geocoding_19methods_full.xlsx")
OUTPUT_ENHANCED = os.path.join(BASE_DIR, "geocoding_enhanced_results.xlsx")

print("="*60)
print("ADDITIONAL GEOCODING EXPERIMENTS")
print("Requested by Professor")
print("="*60)

# ============= HELPER FUNCTIONS =============
def haversine_distance(lat1, lon1, lat2, lon2):
    """Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ·Ï‚ ÏƒÎµ Î¼Î­Ï„ÏÎ±"""
    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):
        return None
    
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    return c * 6371000

# ============= NEW CLEANING METHODS (v20-v30) =============

def clean_v20_km_city1_city2(address):
    """v20: Format: [Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚] Km [Î ÏŒÎ»Î·1] [Î ÏŒÎ»Î·2]"""
    km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    if km_match and len(cities) >= 2:
        km_num = km_match.group(1)
        return f"{km_num} Km {cities[0]} {cities[1]}"
    return address

def clean_v21_km_city1_city2_genitive(address):
    """v21: Format: [Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚] Km [Î ÏŒÎ»Î·1]ÏÎ½ [Î ÏŒÎ»Î·2]Ï‚"""
    km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    if km_match and len(cities) >= 2:
        km_num = km_match.group(1)
        # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î³ÎµÎ½Î¹ÎºÎ®Ï‚ Ï€Ï„ÏÏƒÎ·Ï‚ (simplified)
        city1_gen = cities[0] + "ÏÎ½" if not cities[0].endswith('Î±') else cities[0][:-1] + "ÏÎ½"
        city2_gen = cities[1] + "Ï‚" if cities[1].endswith('Î±') else cities[1] + "Î±Ï‚"
        return f"{km_num} Km {city1_gen} {city2_gen}"
    return address

def clean_v22_km_eo_city1_city2(address):
    """v22: Format: [Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚] Km Î•Î¸Î½Î¹ÎºÎ®Ï‚ ÎŸÎ´Î¿Ï [Î ÏŒÎ»Î·1]ÏÎ½ [Î ÏŒÎ»Î·2]Ï‚"""
    km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    if km_match and len(cities) >= 2:
        km_num = km_match.group(1)
        city1_gen = cities[0] + "ÏÎ½" if not cities[0].endswith('Î±') else cities[0][:-1] + "ÏÎ½"
        city2_gen = cities[1] + "Ï‚" if cities[1].endswith('Î±') else cities[1] + "Î±Ï‚"
        return f"{km_num} Km Î•Î¸Î½Î¹ÎºÎ®Ï‚ ÎŸÎ´Î¿Ï {city1_gen} {city2_gen}"
    return address

def clean_v23_city1_pros_city2(address):
    """v23: Format: [Î ÏŒÎ»Î·1] Ï€ÏÎ¿Ï‚ [Î ÏŒÎ»Î·2]"""
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    if len(cities) >= 2:
        km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
        if km_match:
            km_num = km_match.group(1)
            return f"{cities[0]} Ï€ÏÎ¿Ï‚ {cities[1]}, {km_num}Î¿ Ï‡Î»Î¼"
        return f"{cities[0]} Ï€ÏÎ¿Ï‚ {cities[1]}"
    return address

def clean_v24_remove_all_prefixes(address):
    """v24: Î‘Ï†Î±Î¯ÏÎµÏƒÎ· ÎŸÎ›Î©Î Ï„Ï‰Î½ prefixes (Î Î•ÎŸ, Î•ÎŸ, ÎÎ•ÎŸ, ÎŸÎ”ÎŸÎ£, Î•Î˜ÎÎ™ÎšÎ—Î£)"""
    # Î Î¹Î¿ aggressive Î±Ï†Î±Î¯ÏÎµÏƒÎ·
    patterns = [
        r'Î \.?\s?Î•\.?\s?ÎŸ\.?',
        r'Î•\.?\s?ÎŸ\.?',
        r'Î\.?\s?Î•\.?\s?ÎŸ\.?',
        r'Î Î•ÎŸ|Î•ÎŸ|ÎÎ•ÎŸ',
        r'Î•Î˜ÎÎ™ÎšÎ—Î£?\s+ÎŸÎ”ÎŸÎ¥?',
        r'Î•Î Î‘Î¡Î§Î™Î‘ÎšÎ—?\s+ÎŸÎ”ÎŸÎ¥?',
        r'Î Î‘Î›Î™Î‘\s+Î•Î˜ÎÎ™ÎšÎ—?\s+ÎŸÎ”ÎŸÎ¥?',
    ]
    
    for pattern in patterns:
        address = re.sub(pattern, '', address, flags=re.IGNORECASE)
    
    # ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï‡Î»Î¼
    address = re.sub(r'Î§Î›Îœ|Ï‡Î»Î¼|Ï‡Î¹Î»\.?|ÎšÎœ', 'Ï‡Î»Î¼', address, flags=re.IGNORECASE)
    address = re.sub(r'(\d+)[Î¿Î·ÏŒÎ®ÎŸÎ—]?\s+(Ï‡Î»Î¼)', r'\1Î¿ Ï‡Î»Î¼', address)
    
    return ' '.join(address.split())

def clean_v25_big_cities_pattern(address):
    """v25: Pattern Î³Î¹Î± Î¼ÎµÎ³Î¬Î»ÎµÏ‚ Ï€ÏŒÎ»ÎµÎ¹Ï‚ (Î‘Î¸Î®Î½Î±, Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·, Î Î¬Ï„ÏÎ±)"""
    big_cities = ['Î‘Î¸Î®Î½Î±', 'Î‘Î¸Î·Î½ÏÎ½', 'Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·', 'Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·Ï‚', 'Î Î¬Ï„ÏÎ±', 'Î Î±Ï„ÏÏÎ½', 
                  'Î›Î¬ÏÎ¹ÏƒÎ±', 'Î›Î±ÏÎ¯ÏƒÎ·Ï‚', 'Î—ÏÎ¬ÎºÎ»ÎµÎ¹Î¿', 'Î—ÏÎ±ÎºÎ»ÎµÎ¯Î¿Ï…', 'Î’ÏŒÎ»Î¿Ï‚', 'Î’ÏŒÎ»Î¿Ï…',
                  'Î™Ï‰Î¬Î½Î½Î¹Î½Î±', 'Î™Ï‰Î±Î½Î½Î¯Î½Ï‰Î½', 'Î§Î±Î½Î¹Î¬', 'Î§Î±Î½Î¯Ï‰Î½', 'Î›Î±Î¼Î¯Î±', 'Î›Î±Î¼Î¯Î±Ï‚']
    
    km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
    
    # Î’ÏÎµÏ‚ Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î¼ÎµÎ³Î¬Î»Î· Ï€ÏŒÎ»Î·
    found_big_city = None
    for city in big_cities:
        if city in address:
            found_big_city = city
            break
    
    if found_big_city and km_match:
        km_num = km_match.group(1)
        # Î’ÏÎµÏ‚ Ï„Î· Î´ÎµÏÏ„ÎµÏÎ· Ï€ÏŒÎ»Î·
        cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
        other_city = None
        for c in cities:
            if c != found_big_city and c not in ['Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ']:
                other_city = c
                break
        
        if other_city:
            return f"Î•Î¸Î½Î¹ÎºÎ® ÎŸÎ´ÏŒÏ‚ {found_big_city} {other_city}, {km_num}Î¿ Ï‡Î¹Î»Î¹ÏŒÎ¼ÎµÏ„ÏÎ¿"
    
    return clean_v24_remove_all_prefixes(address)

def clean_v26_small_cities_pattern(address):
    """v26: Pattern Î³Î¹Î± Î¼Î¹ÎºÏÎ­Ï‚ Ï€ÏŒÎ»ÎµÎ¹Ï‚"""
    km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    # Î“Î¹Î± Î¼Î¹ÎºÏÎ­Ï‚ Ï€ÏŒÎ»ÎµÎ¹Ï‚, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï€Î¹Î¿ Ï€ÎµÏÎ¹Î³ÏÎ±Ï†Î¹ÎºÏŒ format
    if km_match and len(cities) >= 2:
        km_num = km_match.group(1)
        return f"Î•Ï€Î±ÏÏ‡Î¹Î±ÎºÎ® ÎŸÎ´ÏŒÏ‚ {cities[0]} - {cities[1]}, ÏƒÏ„Î¿ {km_num}Î¿ Ï‡Î¹Î»Î¹ÏŒÎ¼ÎµÏ„ÏÎ¿"
    
    return clean_v24_remove_all_prefixes(address)

def clean_v27_english_km_pattern(address):
    """v27: English pattern: Highway [City1]-[City2] km [number]"""
    km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    if km_match and len(cities) >= 2:
        km_num = km_match.group(1)
        return f"Highway {cities[0]}-{cities[1]} km {km_num}"
    
    return address

def clean_v28_nomoi_pattern(address):
    """v28: Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î½Î¿Î¼Î¿Ï context"""
    cleaned = clean_v24_remove_all_prefixes(address)
    # Î˜Î± Ï‡ÏÎµÎ¹Î±ÏƒÏ„ÎµÎ¯ Ï„Î¿ countyName Î±Ï€ÏŒ Ï„Î¿ DataFrame
    return cleaned  # Will be enhanced in the loop with county

def clean_v29_only_km_number(address):
    """v29: ÎœÏŒÎ½Î¿ Ï‡Î»Î¼ ÎºÎ±Î¹ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ Î³Î¹Î± testing"""
    km_match = re.search(r'(\d+)[Î¿Î·ÏŒÎ®OH]?\s*(?:Î§Î›Îœ|Ï‡Î»Î¼|Km|km|ÎšÎœ)', address, re.IGNORECASE)
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    if km_match and len(cities) >= 1:
        km_num = km_match.group(1)
        return f"{km_num}Î¿ Ï‡Î»Î¼ {cities[0]}"
    
    return address

def clean_v30_inferred_highway(address):
    """v30: Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± Î½Î± ÏƒÏ…Î¼Ï€ÎµÏÎ¬Î½ÎµÎ¹ Ï„Î·Î½ ÎµÎ¸Î½Î¹ÎºÎ® Î¿Î´ÏŒ"""
    cities = re.findall(r'[Î‘-Î©A-Z][Î±-Ï‰a-z]+', address)
    keywords = {'Î§Î›Îœ', 'Km', 'Î Î•ÎŸ', 'Î•ÎŸ', 'ÎÎ•ÎŸ', 'ÎŸÎ”ÎŸÎ£', 'Î•Î˜ÎÎ™ÎšÎ—Î£'}
    cities = [c for c in cities if c.upper() not in keywords]
    
    # Known highway patterns
    highway_map = {
        ('Î‘Î¸Î®Î½Î±', 'Î›Î±Î¼Î¯Î±'): 'Î‘1',
        ('Î‘Î¸Î®Î½Î±', 'Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·'): 'Î‘1', 
        ('Î‘Î¸Î®Î½Î±', 'ÎšÏŒÏÎ¹Î½Î¸Î¿Ï‚'): 'Î‘8',
        ('Î‘Î¸Î®Î½Î±', 'Î Î¬Ï„ÏÎ±'): 'Î‘8',
        ('Î›Î¬ÏÎ¹ÏƒÎ±', 'Î’ÏŒÎ»Î¿Ï‚'): 'Î•ÎŸ3',
        ('Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·', 'ÎšÎ±Î²Î¬Î»Î±'): 'Î‘2',
    }
    
    if len(cities) >= 2:
        for (c1, c2), highway in highway_map.items():
            if (c1 in cities and c2 in cities) or (c2 in cities and c1 in cities):
                km_match = re.search(r'(\d+)', address)
                if km_match:
                    return f"{highway} {km_match.group(1)} km"
                return f"{highway} {cities[0]} {cities[1]}"
    
    return clean_v24_remove_all_prefixes(address)

# Dictionary Î¼Îµ Ï„Î¹Ï‚ Î½Î­ÎµÏ‚ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï…Ï‚
NEW_CLEANING_METHODS = {
    'v20_km_city1_city2': clean_v20_km_city1_city2,
    'v21_km_city1_city2_genitive': clean_v21_km_city1_city2_genitive,
    'v22_km_eo_city1_city2': clean_v22_km_eo_city1_city2,
    'v23_city1_pros_city2': clean_v23_city1_pros_city2,
    'v24_remove_all_prefixes': clean_v24_remove_all_prefixes,
    'v25_big_cities_pattern': clean_v25_big_cities_pattern,
    'v26_small_cities_pattern': clean_v26_small_cities_pattern,
    'v27_english_km_pattern': clean_v27_english_km_pattern,
    'v28_nomoi_pattern': clean_v28_nomoi_pattern,
    'v29_only_km_number': clean_v29_only_km_number,
    'v30_inferred_highway': clean_v30_inferred_highway,
}

print(f"\nâœ… ÎŸÏÎ¯ÏƒÏ„Î·ÎºÎ±Î½ {len(NEW_CLEANING_METHODS)} Î½Î­ÎµÏ‚ Î¼Î­Î¸Î¿Î´Î¿Î¹ (v20-v30)")

# ============= MAIN EXECUTION =============

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· existing results
print(f"\nğŸ“‚ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· existing results Î±Ï€ÏŒ: {EXISTING_RESULTS}")
df_existing = pd.read_excel(EXISTING_RESULTS)
print(f"   Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {len(df_existing)} ÏƒÏ„Î±Î¸Î¼Î¿Î¯")
print(f"   Existing columns: {len(df_existing.columns)}")

# Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î½Î­Ï‰Î½ ÏƒÏ„Î·Î»ÏÎ½
print("\nğŸ”„ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î½Î­Ï‰Î½ geocoding experiments...")
print(f"   API calls: {len(df_existing) * len(NEW_CLEANING_METHODS)}")
print(f"   Î•ÎºÏ„Î¹Î¼ÏÎ¼ÎµÎ½Î¿Ï‚ Ï‡ÏÏŒÎ½Î¿Ï‚: ~{len(df_existing) * len(NEW_CLEANING_METHODS) * 0.1 / 60:.1f} Î»ÎµÏ€Ï„Î¬")

# Progress bar
for idx, row in tqdm(df_existing.iterrows(), total=len(df_existing), desc="New Geocoding"):
    
    for method_name, clean_func in NEW_CLEANING_METHODS.items():
        try:
            # Special handling Î³Î¹Î± v28
            if method_name == 'v28_nomoi_pattern':
                cleaned_address = clean_v24_remove_all_prefixes(row['original_address'])
                cleaned_address = f"{cleaned_address}, ÎÎ¿Î¼ÏŒÏ‚ {row['countyName']}"
            else:
                cleaned_address = clean_func(row['original_address'])
            
            # Geocoding query Î¼Îµ county
            query = f"{cleaned_address}, {row['countyName']}, Greece"
            
            # API call
            result = gmaps.geocode(query, region='gr')
            
            if result:
                loc = result[0]['geometry']['location']
                distance = haversine_distance(
                    row['ground_truth_lat'], row['ground_truth_lng'],
                    loc['lat'], loc['lng']
                )
                accuracy = result[0]['geometry']['location_type']
                
                # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
                df_existing.at[idx, f'{method_name}_address'] = cleaned_address
                df_existing.at[idx, f'{method_name}_lat'] = loc['lat']
                df_existing.at[idx, f'{method_name}_lng'] = loc['lng']
                df_existing.at[idx, f'{method_name}_distance'] = distance
                df_existing.at[idx, f'{method_name}_accuracy'] = accuracy
            else:
                # Î‘Ï€Î¿Ï„Ï…Ï‡Î¯Î± geocoding
                df_existing.at[idx, f'{method_name}_address'] = cleaned_address
                df_existing.at[idx, f'{method_name}_distance'] = None
                df_existing.at[idx, f'{method_name}_accuracy'] = 'FAILED'
                
        except Exception as e:
            print(f"\nâŒ Error Î³Î¹Î± station {row['gasStationID']}, method {method_name}: {e}")
            df_existing.at[idx, f'{method_name}_address'] = cleaned_address
            df_existing.at[idx, f'{method_name}_distance'] = None
            df_existing.at[idx, f'{method_name}_accuracy'] = 'ERROR'
        
        # Rate limiting
        time.sleep(0.1)

# ============= FIND BEST METHOD OVERALL =============
print("\nğŸ“Š Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÎºÎ±Î»ÏÏ„ÎµÏÎ·Ï‚ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï… overall...")

# Î£Ï…Î»Î»Î¿Î³Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î¼ÎµÎ¸ÏŒÎ´Ï‰Î½ (Ï€Î±Î»Î¹Î­Ï‚ + Î½Î­ÎµÏ‚)
all_methods = []

# Î’ÏÎµÏ‚ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï…Ï‚ Î±Ï€ÏŒ Ï„Î¹Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚
for col in df_existing.columns:
    if col.endswith('_distance') and not col.startswith('best_') and not col.startswith('original_'):
        method_name = col.replace('_distance', '')
        all_methods.append(method_name)

print(f"   Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î¼Î­Î¸Î¿Î´Î¿Î¹: {len(all_methods)}")

# Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ best method Î³Î¹Î± ÎºÎ¬Î¸Îµ ÏƒÏ„Î±Î¸Î¼ÏŒ
def find_best_method_enhanced(row):
    """Î’ÏÎ¯ÏƒÎºÎµÎ¹ Ï„Î·Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î¼Î­Î¸Î¿Î´Î¿ Î³Î¹Î± ÎºÎ¬Î¸Îµ ÏƒÏ„Î±Î¸Î¼ÏŒ"""
    best_method = None
    best_distance = float('inf')
    
    for method in all_methods:
        dist_col = f'{method}_distance'
        if dist_col in row and pd.notna(row[dist_col]) and row[dist_col] < best_distance:
            best_distance = row[dist_col]
            best_method = method
    
    return pd.Series({
        'best_method_enhanced': best_method,
        'best_distance_enhanced': best_distance,
        'improvement_over_v1': row['v1_original_distance'] - best_distance if best_method else 0
    })

# Î•Ï†Î±ÏÎ¼Î¿Î³Î®
best_results_enhanced = df_existing.apply(find_best_method_enhanced, axis=1)
df_final = pd.concat([df_existing, best_results_enhanced], axis=1)

# ============= STATISTICS & RANKING =============
print("\nğŸ“ˆ Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±Î½Î¬ Î¼Î­Î¸Î¿Î´Î¿:")

stats_data = []
for method in all_methods:
    dist_col = f'{method}_distance'
    if dist_col in df_final.columns:
        distances = df_final[dist_col].dropna()
        
        if len(distances) > 0:
            stats_data.append({
                'Method': method,
                'Mean_Distance_m': distances.mean(),
                'Median_Distance_m': distances.median(),
                'Success_Rate_%': (len(distances) / len(df_final)) * 100,
                'Within_100m_%': (distances <= 100).mean() * 100,
                'Within_500m_%': (distances <= 500).mean() * 100,
                'Times_Best': (df_final['best_method_enhanced'] == method).sum()
            })

stats_df = pd.DataFrame(stats_data).sort_values('Mean_Distance_m')

print("\nğŸ† TOP 10 ÎœÎ­Î¸Î¿Î´Î¿Î¹ (by mean distance):")
print(stats_df[['Method', 'Mean_Distance_m', 'Within_100m_%', 'Times_Best']].head(10).to_string(index=False))

# ============= SAVE RESULTS =============
print(f"\nğŸ’¾ Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· enhanced results ÏƒÏ„Î¿: {OUTPUT_ENHANCED}")
df_final.to_excel(OUTPUT_ENHANCED, index=False)

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÎºÎ±Î¹ Ï„Ï‰Î½ statistics
STATS_OUTPUT = os.path.join(BASE_DIR, "geocoding_methods_statistics.xlsx")
stats_df.to_excel(STATS_OUTPUT, index=False)
print(f"   Statistics Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î¿: {STATS_OUTPUT}")

# ============= FINAL SUMMARY =============
print("\n" + "="*60)
print("Î£Î¥ÎÎŸÎ¨Î— Î‘Î ÎŸÎ¤Î•Î›Î•Î£ÎœÎ‘Î¤Î©Î")
print("="*60)

# Overall improvements
original_mean = df_final['v1_original_distance'].mean()
best_mean = df_final['best_distance_enhanced'].mean()
improvement_pct = ((original_mean - best_mean) / original_mean) * 100

print(f"\nğŸ“ ÎœÎ­ÏƒÎ· Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ·:")
print(f"   Original (v1): {original_mean:.1f} m")
print(f"   Best method:   {best_mean:.1f} m")
print(f"   Î’ÎµÎ»Ï„Î¯Ï‰ÏƒÎ·:      {improvement_pct:.1f}%")

# Best performing new methods
new_methods_performance = stats_df[stats_df['Method'].str.startswith('v2')].head(5)
if not new_methods_performance.empty:
    print(f"\nğŸŒŸ ÎšÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Î½Î­ÎµÏ‚ Î¼Î­Î¸Î¿Î´Î¿Î¹ (v20-v30):")
    print(new_methods_performance[['Method', 'Mean_Distance_m', 'Times_Best']].to_string(index=False))

print("\nâœ… Script Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
print(f"   Enhanced dataset: {OUTPUT_ENHANCED}")
print(f"   Statistics: {STATS_OUTPUT}")
